Hey, hi everyone. Welcome to this line chain drag crash course. In this particular course, which is a two part series course, we're gonna build a conversational drag basically a drag chatbot. And this will be a conversational and also it will support multiple users. Basically that's the requirement we have in the production if you deploy a drag chatbot. We will see how we will go from this, you know, in notebook, a Jupyter notebook to creating the most structured application as an API. And then this drag API then will be you know, utilized by any front end. In our case, we gonna use our own stringed application, which will communicate with the, you know, the API that we have built for our drag chatbot. And you can see this particular, you know, API has the document management. Basically you can add more documents to your, you know, drag application, you can delete those documents and then basically you can chat with this. As we know all machine learning projects or let's say all the things what we do it starts with the collab notebook or the Jupyter network. And that's where we're gonna spend most of our time because we're gonna see how things work here. So let me show you what exactly, you know, they gonna cover in these two parts series. So this will be a part one in this part we will go and we will understand what exactly the drag. We will not go much into the theory, but most like a practical implementation and understand what exactly the drag application. We will look at the line chain and the line chain expression language. Basically the line chain component that makes us easy to create this LIDM application and we will explore them. Then we will see how do we process the documents using the line chain utilities, how do we create the vector database basically a semantic search and then we will step by step build our basic drag chain. Once we have our basic drag chain, we will make it a conversational drag. The conversational means the user is able to ask a follow up question, the chatbot understand the previous questions and the interactions and then answering pace on that. So we will see how do we refine and contextualize the follow up question so that it works well with the drag application. And we will also see some of the pre-built line chain, you know, drag chains that we could use to build this kind of application. And finally, we will see how we can make this, you know, a drag chatbot and a multivariate chatbot. So we will see how we can store the conversation history or all the sessions information in the database. And then once we have all these things ready, then we will see how can we take our this notebook and productionize it basically converting this notebook into an API. So we will use the first API and we will expose those endpoints what I, you know, just shown here and this endpoints then basically can be used by any front end. This is how reality, you know, things box. So as a freelancer, we have our features, where we build a custom drag or, you know, the element applications for our client. Most of the time, the delivered application is an API and then our clients can use this API and they can integrate with the any application or any front end they want to use. In our case, you know, we are going to use the stringlet as a front end and the stringlet going to communicate with this particular API. So this is, you know, what we are going to cover in this particular, you know, crash course. As I said, we will not spend much time on the, you know, theory, but we will see what exactly the retrieval augmented generation, right. So we will touch upon each of these words. But so let us first understand what exactly retrieval augmented generation or the drag and why it is important. Each of us has used some kind of an element like a chat GPT, right, where we could ask something and chat GPT answers us. And this is nothing but the generation. So it is actually generating the output or the content for you. That is what we call them as a generation model. And that is one of the thing in the, you know, drag which is the generation, right. But that generation is getting augmented by the retrieval and we want to see what exactly that means. So if you look at any typical rack system, let me show you some, you know, diagram which is from the line chat website. So this is what things happen. So we got some question that we want to ask the LLM just like what I asked here. And that question can directly we can pass the LLM and LLM can answer. This is what we did here. But most of the time what happens, what we are asking maybe LLM is not aware of that information. Maybe LLM has been trained it, you know, two years back and there are a lot of new things that happen. So basically it doesn't have in for, you know, don't have access to the latest information. That is one thing. Other thing is that this particular LLM don't have access to your proprietary documents. So in your company, in your system, you have many documents and this document has a lot of important information. That's the knowledge what you have about your application, your company. And this knowledge, this external knowledge is not accessible to the LLM. So RAC is nothing but is bringing this retrieval to this particular generation. So what exactly we are doing when we ask a question, we first retrieve some meaningful important information or we call it context to answer this particular question. And then rather than just passing only question in the prompt, we take the question and the retrieve document, we call it context, these both things go and then LLM is able to answer. So this way we are able to augment the LLM with the retrieval and that's what we call it, you know, the retrieval augmented generation. We are retrieving our own documents and then we are augmenting, you know, we are putting them inside the prompt and then we are getting the jump ticket output. And that's the whole idea of RAC and which is very much important because you will never have an LLM which is up to date, right. And you always want or you might not want to expose your data to everyone. So you definitely want to require some kind of a system, you know, that can give LLM what is needed to answer the question. So that's basically, you know, retrieval augmented generation. And typically the process in was, you know, so you got a lot of documents, we will see this inside the code. Let's say you have a lot of documents, you need to state this document into the small small part, let's call it chugs. And then each of these chugs, we convert into there something called a vector representation using the embedding. And then this gets stored into there something called vector store or vector database. These are the specialized database which are good at storing this large, you know, vectors and we could use them for the semantic search. So basically if I ask any question, how do I make sure I get the relevant documents where you know, retrieved here. So this is the small theory about, you know, what exactly the retrieval then the augment and then finally, generation begins which is basically the RAC. Now let's get into the code and you know, we do all these things what we know, just discuss or I show you what we can have covered. So let's do the first thing, you know, let me install the required, I think I already run. So let's see what we have installed here. So we have installed the line chain then there is something called line chain opening. So if you are not familiar with the line chain ecosystem, we can see what does the line chain provide. So basically the line chain is a framework, you know, using which we can build a LLM where the large language model is application. And line chain has to offer a lot of things. So basically it has a line chain. There is something called line graph which is basically you could build a AI agent using the, you know, this line graph and you could even deployed on their cloud. If you want to see what exactly the core component it has of something, first thing is has is a line chain code. Basically this is the line chain expression language, we're going to see it what exactly it means basically all the LLM, the, you know, the prompts, all those things that come under this line chain code. And there is something called line chain community each of this community packages like let's say, line chain has some integration with the open AI or maybe anthropic or the chroma DB. So each of them have their own packages. So that goes under the line chain community. And finally, the, you know, the line chain which is nothing but those chain the agents or all these things what it has then line graph is for those building the agent. And finally focusing on only the line chain, the, the line chain part where we have building this drag application and don't worry, we're going to see exactly what this line chain expression language and why it is, you know, really go first upon why do I use the chain because I use this frequently right. And the thing is I like like about the line chain is basically it brings some structure to our application and the second thing is it become easy for me to switch between the different language different LLM right I could easily switch between the open AI from anthropic and I don't worry about it. Other thing I like about the line chain is the line smith because you know, length smith is kind of a monitoring and you know debugging tool where we could see what's happening under the hood and we're going to see this thing throughout this course all things we are learning we're going to start with the basic and we're going to understand what's happening behind the scene whenever you learn any chain what's happening under the hood. So, I want to see that thing also right and so the primary reason at least for which I use the line chain is first is the line smith I like this debugging tool I could see what's happening there to the hood and that has improved my understanding of line chain also and the line chain itself because it makes you know easy for me to switch between the LLM's and you know it also brings some structure that my team can follow let's say everyone follow. Let's go back to you know the collab notebook and let's do what is the next okay first of all let's bring this line chain version so that you when you refer this particular thing you know notebook as I always share my collab notebook or is a block post or maybe I upload on key that so don't worry you know you don't need to worry about this code you will have access to this particular code. Next thing you could say I'm using open a key because I'm going to use the open a LLM now here in this tutorial I'm showing you my key here right usually you should really as a some kind of a you know sick or something like that but I just kept it here after this recording I'm going to delete this particular key so you won't be having access to this key. So we have the open a key because we want to use their LLM and then as I mentioned we want to use this line smith and just within a few minutes you will see what exactly the line smith and to how that line smith you know we also require the line smith key that you get from the you know if you go to the I guess let me show you if you go to the line smith you know you can get that key and I think where it is is I think go to the app. Yeah basically here you can log in and you can get here you know you are going to see this thing as we're on the program and what next yes so basically you know we're going to give some name to that line chain course because this is where our all logs will be stored. Okay so let's start with the simple thing how do we call an LLM inside the line chain because that is the basic of you know all what we're going to do. So basically in the line chain open as I mentioned this is the let's say line chain integration or the line chain community package we can import the chat model so chat models are nothing but the ones which can accept the conversation messages to something right so we can import that chat model and then we can specify which model we want to use so we get the model instance let's say call LLM and then we can invoke this model by passing some query just like we pass here right so where it is so I just ask something to this LLM. This asking here is nothing but invoking this and we will get some kind of a response here so this is the basic syntax to use an LLM inside the line chain and keep this particular syntax in mind dot invoke because that's the beauty of language which allows us to each of this line chain component here is this LLM that we can invoke and see what output it produces. So let's run this particular thing. Okay so we got something here when I run this LLM by passing some input to it let's say I'm asking tell me a joke I got response of a type called AI message and this message has something called content where is the actual string response I have so this is some kind of a joke that you know you know it has created. So let's see what has happened under the hood that's what I told you we will see inside the line Smith what's happening. Okay so we are inside this you know the line Smith you can see this is the line Smith you know tracing where we have all the runs and I have multiple project but I am currently inside the line chain course because that's what the name we have given. If you look at here what name we have given for tracing is this line chain course and that's the reason I am getting this you know so I just called an LLM so whenever you call any change so even LLM is a part of it like it's let's say it's a line chain component so when I call this thing I could see what has happened under the hood here. Each call you can see this is a chat opening call and if I click here we see that that human has passed a tell me a joke and we got something called type AI which is the message that you see right so if you look at here we got a reply or a message of type called AI message so this is what the AI message and we have a message that is what the message is. And whatever we have passed sorry whatever we have passed here that has become the human message so this all LLM is taking the message as input and producing messages and output that's what we saw but when we work with you know the application we don't know this you know AI message type right we want the plain takes or the string output which is this one so that's where the parsing comes into pictures so line chain course. So line chain also has the output parser so there are different output parser are available we could use those parsers to actually parse our LLM response it will take this AI message which is the LLM response and convert into the plain string what the takes representation what we have so let's create the output parser object and then let's call let's take the LLM response that we got and we will pass through the output parser and let's see what we get. We can see we got the same plain takes what we just saw above so we saw LLM which produces an AI message and then this AM message we passed the parser and eventually we got the plain takes output let me say whether this thing is also recorded into the lines meet or not so it's good the lines meet and yes you could see we also have this particular run has recorded into the lines meet. So we can see what is happening here if I click on that input was the AI message that's correct because we had pars the you know the model response which was the AI message and the output you can see there is no message that it's a plain takes output compared to what we saw earlier earlier when we call the LLM the input was the message of type human and the output of type message AI here it is the same as the one we saw earlier. So basically we are passing the output of LLM to the parser and that is the reason you know we could put this in a better way and that's nothing but the line chain chains. So let's create a simple chain chain is nothing but let's say you can put this line chain component in a sequential manner and you can execute them because we saw first we have to pass through the LLM and then output of the LLM then it's to pass through the parser then why not put them at the single chain. So to make the chain we put the line chain components you know and then despite symbol and then the another components basically if I call this chain whatever thing I'm going to pass it will first go through the LLM and then it will go to the output parser and should do the same thing if I'm calling them independently. So let's run this thing and we will see again inside the line smith what has happened. So let's go inside the line smith. Okay, earlier this saw when we run them independently we got two entries here but now we have run it is a part of one sequence. So you see here runable sequence. Now let's click on that and we could see what has happened. Now this runable sequence has the two steps the first we are calling the open AI chat open AI and then we are calling the output parser. These are the nothing but the two steps what we saw first we are calling the element then we are calling the output parser and we could see here what's happening. So when I click on this you know open AI call the human message came AI message got the you know produce and then next step I could see that same AI message came and we got the plain text. This is nothing but the line chain we are able to form a chain which has done these you know two things producing the output and parsing the output but you know most of the time when you work with the real application you really don't want the plain text output you rather want output should have some structure so that you can manipulate that output. Now what exactly that mean. Let's take an example let's say we got some review you know this is like a mobile review and we want to analyze this review and give us the structured output for that we want to know which phone model it is talking about what it's rating what pros cons and summary what user is saying basically right to define that structure we use the identity you know if you guys are familiar with the even the fast API to define the you know the models you know identity models. Or the structure to the request and response we also use the identity so using the identity model we can define the structure so here we are saying we want to get the output as a let's say the mobile review it's a type of the output and these are the fields it should have let's see how we can make sure the LLM is able to produce the output in this particular format. Just like we are calling LLM you know earlier we are calling LLM using the invoke method here before we call that you know LLM we can use its method called with the structured output so whenever we want output as a you know some kind of a structured like this kind of type we can use this with structured output method and we can pass our you know the identity class that which is created which is basically a model that you know sorry it is a very good way to do it. Sorry a type that we want to create out of this LLM response and then similar way we put invoke the LLM on this newly let's say you know LLM which is having the information about our structured output now let me you know run this thing sorry. Okay let's see what happened we got an output of type called mobile review because we have mentioned this is what we expect in the output and this mobile review has all this attribute that we are specified the phone model the rating the pros and the cause which is extracted from this particular review and this is very much important because once you know what is output is made of you can extract and manipulate each of its fields what you are specified so let's say here. I just want to know what are the pros so since the output is the variable called output and I know that it's of a type called mobile review and it has a field called pros and I can simply access it and I could see this list of the pros you know what is structured this is I mean reality we build the application even if you build the natural language to a spell application we might want a specific structure that I want a spell query that I might want some explanation or something like that. So that you have control what output it is produced okay now what next till the point you know we got the LLM we got this parser but this input what we are passing is a static but that's not how you know in reality things are dynamic you know the inputs has a variables of something called the placeholders and we're going to see how does launching allows us to specify those placeholders. So, launching has something called the prompt template so basically templates are nothing but those streams with the placeholders so this is coming from this let's say launch and core so as I told you when we saw here you know there are some things called so the prompt LLM and all those components are specified inside this launch and core again we don't need to remember this thing so from the launch and core we are implementing sorry importing the chat prompt template now since we are dealing with the chat model you know this open AI chat chip it is a chat model basically it is taking a message or the conversation as an input we saw even if you put the normal string like this it is converting into the message because it is expecting a message. And this is how we can specify a prompt template where we have a string with the placeholder and then we can pass this placeholder value when you are actually calling the in a model but before we call this prompt through the LLM since there is you know something called live chat expression language what it makes each of these components what we saw we could invoke those each of just that compel and see how it behaves right. We saw we could invoke the LLM we could invoke the parser we could even invoke the chain because all of them follow the same interface similarly before even executing the chain we will just invoke this prompt and see if I am passing value for this you know placeholder so I can pass it as a dictionary I am passing the key value pair because there is one value variable called topic and I am passing a value for it called programming. Now let's see this prompt get modified or not this way we are able to make the dynamic prompt which has the placeholders so the output is something called chat prompt value which is basically nothing but your template with the value substituted and as we guess you know it is a type of call human message and it is producing the list of messages because chat model can take the list of messages which represent the conversation and here we could see the content tell me a joke. About and this variable topic got replaced with something got programming this way you know using the chat prompt template we can make our prompt dynamic because as we are building the rack application we saw here somewhere right that final prompt will be augmented with the information we got here so in this particular prompt we need to have some placeholder for the question some placeholder for the context and maybe some history if you want to make it a conversation. And all these things we are going to require there. Now let's go back to our you know building that line chain chain because we are putting each component step by step. Now I want to improve our chain because rather than passing a hard coded fix prompt to the chain I want to pass a template and then make a prompt out of it. Let's say I want to pass the placeholder and make a prompt out of it so the first thing I want to do is to create the prompt and then I want to you know the prompt is nothing but this the one we define at the top. Then I want the output of the prompt basically this thing should go to the lm and then finally it go to the output parser. Now again this whole chain I can execute this because this chain is expecting a placeholder all a topic. And let's invoke and see what's happening under the hood we can go back to now lines with now we got the answer like we got you know we ask about now car drivers and we got the joke about the car drivers right. So you could change the topics and you know we could get this you know it could be what is this car drivers maybe I could say programmer. And we should get some forget about the spelling mistake you know okay why do programmers prefer dark mode because light attracts the bugs so this is some you know joke about the programmers. So this is the beauty of the length and expression line where this is what I call a bringing structure to your application you know we can specify the prompt and prompt goes to lm and lm's output get parsed in separately. Now this everything has to be a one single run in our you know length so let's go and check there if I come here in the length and look at the you know so we got what yeah so this is so you could see first we run with the programming then we run with the car drives and now we run with the programmer. So let's look at the one run expand it okay now we could see earlier we have we saw these two tapes two steps chat model and the output part but now we have in front of it a prompt template where we pass a you know template we pass the topic is equal to programmer and then it converted into this message tell me a joke about the programmer let me see in a different way we could see different. So we pass this programmer as a topic and it get you know now this will be something that is going to the lm so let's go to the lm and we could see tell me joke about the programmer and then we got this a and sation that is going to parser interesting right this is how we build the chain. Now let's again go back to the collaborative look and let's keep building on top of it now let's say it I just put all these six together a simple thing a chain here we are putting and it does the same thing in a what we just saw. We just put all this thing in a you know single sale here next what we saw this different messages coming right we saw a message we saw human message so whenever we put some input it's actually a message from the user which is a human message. Now let's look at we could create those message our self custom right for example you know from the 9 chain code we could input the message type call human message and the system message so basically you know system message usually define the character or the behavior of your agent or the bot what it is you know you are creating so usually you will see some kind of you know. Character specifying instruction there for example you are a helpful assistant that tells you or you know be polite some kind of a you know character specification character you put it into the system message you can put your context of the system message no problem but this is usually the use of the system message to give some instruction that will define the character of this particular you know bot what we are creating and we could create the system message but because we saw each of this message you know whenever we saw the message. They have filled call content and the type of the message which is this so here also we are creating a system message specifying the content that you know this is thing then we are creating a human message that tell me about programming because it is about the jokes and then as we know that the LLM can take the list of messages so we can you know put this thing so this is how you could build a custom messages you know system message you know. This is important we are going to build this thing when we build the right application so we are passing that first is message as a system message second message is human message and we are calling the same LLM that it should be able to do the similar job. So this way also we could pass a conversation as a list of messages and we got the messages you know could because we have not used the output parser and we can once again go and check into the you know here and we should be able to see that it took the two messages the first one is the system messages then the second was the human message and then it produced the AI message. Okay now next what we are going to do we are going to see a different way of creating the prompt template so the first way we saw creating the prompt template is basically a string with some kind of a placeholder right but then we could also create the you know the prompt with a list of messages just like what we did here because we know LLM accept the list of messages so this was the one of the way to create the system messages using this you know this class system message class. Or you could also specify one message as a tuple where the first part in that tuple specifies the type of the message and the second part specifies the content of that message which is this one you know contained and there and we could do the same thing what we just did here and you could also invoke this particular prompt using the same way and we can call this you know LLM similar way. So these are the different ways actually you could construct the prompt using you know this tuple syntax using this system messages of human messages this way we can create this now this whole part what we saw is basically the line chain expression language the line chain expression language you know which allows us to put this different different line chain components like LLM the output parser the prompt and create the chain and they all follow this in the language. So this is the way we can extract this syntax like invoking method that's why it makes us easy for us that you know to orchestrate or to put them in a sequence you know in a way that we want them to be executed. Now let's see how we can extend our this small chain to do a rack because till the point we saw all the knowledge or the output we got it from the LLM its own knowledge and not from the external source of information what we have. So let me see whether I have uploaded some documents here. Okay so here is the docs folder what I created and I uploaded couple of documents here you know some company and their information basically that some company called green group. Now I'm not sure whether they exist or not these are all five documents charge GPT generated so I asked simply charge GPT you know give me some fictional document that I can use for the rack and when I'm going to share this particular document or you could use your own in your documents. Now let's go towards the rack but we know to do this you know the rack thing which is basically retrieval we need our documents to be present into the vector store here and you can't just put or dump them you know you need to process this document so those documents could be different type it could be a pretty a doc X or some other document type. So you need to process you need to read those documents split it into the smaller chunks and then the small chunks we could embed basically we could convert it into the vector and this chunks goes here. So when retrieval happens we bring the matching chunks the chunks which are relevant to the questions that user is asking. So let's focus on this part now how do we load those document how do we split them embed them and store into the vector database and how do we retrieve them you know. So let's see the step by step let me go back to again you know the code so this is the code which is related to the processing the document. Now in the line chain community you know we have different document loaders so here I have mostly PDF document and the doc X document you know PDF and the doc X. So I'm going to use you could go and check what are the different document loaders line you like in support I'm going to use the PDF folder I am going to use the you know doc X to TXT loader and there are also different split or how do we you know split this particular document so we just saw here that we need to split those documents there are different ways to split right how do we do want to split on you know some paragraph level or they so this up there are different different in a split that you could use let me see the. You know if you go to the text pleater you can read more about them you know so one of them we are using is this recursively you know so first it starts with splitting a text by the you know paragraph if it couldn't find the paragraph it will go and try to split by the new line if you couldn't find the new line it will try to split by the space you know so it will try to split recursively you know from paragraph to let's say this new line to do something so we're going to use this one and this is also the default one what. And so this is a and we will see what it is doing so we're going to use this recursive character and then we also require some embedding right so we will see both the embedding sentence transformer and also the opening embedding let's look at the open a embedding we're going to import from the line to an open a embedding and first we do is we create the object of this split. So we create the split and we can specify hey I want to keep my chunks ice to be a thousand characters then we know that this technique can split it anywhere it might happen that you know it split on some important point where half of the point goes to one chain the other goes to other chunk to avoid that thing we will have some overlapping between the two chunks so at least you know some important thing what we have is present in both the places you know if it split arbitrarily somewhere and that's where the role of the overlap so we will have 200 character overlap between the things so both of it will them have that comment and basically the length is I guess the length maybe you could specify the length parameter differently I haven't you are much thought about it so this is what we have okay let's see how we can load a single document and split into it or split it so I'm going to pick up one of my document which is let's say doc X by what we have here so I'm going to use this doc X load up and going to pass the path of the fine I get the loader and then I can call a method called load and then I will get the documents so here I actually got the you know list of documents and then I could use this splitter you know which will split currently this this should be as one document because we have only one document so let's print it. Let's print the length of the document how many document we got here. We should be one because there is only one document and then we split and we can see how many chunks we get from it okay so there was only one document and we got two chunks right and they can look at each of these chunks so this is how each chunk looks like so each chunk is also a document okay so basically it's a small document and the one we saw this should be also a document so let me see what it is so if I go inside this document we know there is only one document So this is also a document only this is a bigger document because it has everything but we split into the smaller document this is one document there's another document if you let's say create the smaller one let's say I create the 500 character then I should get the smaller you know I got now six chunks out of instead of you know and this only the 500 character chunk Oh sorry this one is a bigger one and thus this is the small document which is chunk but let's say you can experiment but ideally 1000 2000 characters are enough so let me put it 1000 you know character and let's do it again so we got these two chunks created and we just saw one of these chunks each of this chunk is nothing but a document which has something called metadata a key value pair here you could see the source it has stored from which document this chunk came from and also the page content this is the actual content of that chunk the text what is associated and we care about this page content when we pass this information inside the prompt and we could see them separately this is the metadata and this is a page content here properly right okay so where are we we loaded our document with split but we just did with the one document so usually we will have multiple documents with the different you know data type so what I did I wrote a small utility function here for loading the documents which take a folder path in this case the doc and based on its let's say data type not data type the file type if it is PDF it will use the pipedient loader if it is t doc x it can use this if it is html you can use some different loader because it's support lot of loader but frankly I have only two so let's call this function by passing folder path which is our docs so it's all the document let's call it okay so we total got the five documents from the folder 1 2 3 4 5 so five documents are there and total we got eight chunks out of this five documents now let's embed those all this document as I mentioned first we gonna try the open a embedding but we could also use the sentence transformer embedding so we can create the open a embedding instance and we can call a method called embed documents where we have to pass our you know a page content so we know each three thousand content right page content so we are passing the list of page content here and we are getting this output you know document embedding so it has embedded the eight documents basically this is nothing but the list of page content that we are passing and let's look at how each embedding looks like so embedding is nothing but this whole you know a vector representation of our first chunk that's what so we got eight chunks this is the first embedding of the first chunk and we reach to this point now we got this embedding now we need to store this embedding but before that what if we want to use some different embedding right so open AI is one of the option and we could use the open source embedding else so from the line chip community we could also use the sentence transformer embedding let's say I'm using this particular model and you could do the same thing and we should be able to get the embedding here also okay we got some deprecation warning maybe it is downloaded this you know the model in which I just specified okay it's it's now downloading that model and okay here we got the sentence transformer embedding so you could use whatever the embedding you want right basically Langchen is all about putting this Langchen component now we just saw the embedding you know then we also saw this we will see now something called the retriever also or the you know weekly data so let me clear this output okay so we got this embedding now we want to store them here in the vector store so we're gonna use the chroma DB as a vector store so from the Langchen Chroma we got to import the Chroma for this purpose let's say I'm using the open AI embedding I could use any one of them I'm just giving some correction name so if you're not familiar with the you know I also have a couple of videos but they might be old now but you know the vector store you could store our documents and chunks you know you could watch some other video if I get chunks I will just put some other reference video there and you could take this vector database whatever right and we can put our documents so basically in the Langchen you know we can create this vector store from our documents and we could give some name to it let's add you and some name called collection and we can give our document documents are nothing but all our chunks because those chunks are itself are documented right these are all the things what we got so we got this split here so we got eight chunks this splits and we are putting those splits inside the vector store called Chroma okay and did we run this thing no so let's run this thing so that we create this vector database okay so vector database is created maybe we could see the database coming here yeah this is the chroma DB vector database so we got this all documents and their chunk and they're inside this particular chroma DB mixed let me ask some questions regarding the documents that what I have so we saw this document there is some documents of green, grows and I know fictional thing and we want to see since it is a vector store we should be able to retrieve so we are just talking about the closed system we don't need this one we're going to talk about now retrieval if I pass the question and these are my documents stored I want to retrieve something meaningful so let's see whether we are able to find the you know meaningful document so let me ask this question then was the green grow innovations founded and I want to find the two matching document from this vector store let's see what we get so we could use a method called vector store as a method called similarity search or the semantic search you can call it we have this query and we wanted to find the matching documents from our vector store and we got these two documents so we because we we ask for the two documents you know k is equal to doing the similarity search so this is the first you know document where we are printing the source of the document and we are also printing the content of the document so source seems to be green grow company and the second source is also from the same so we got some file file a different files and we could see it has you know chosen the matching chunk which is relevant to the green row that's a good thing and I guess one of them even have the information about when it was founded so basically semantic search is working good out of all these chunks it is able to find out the green grow when the green grow innovations founded the matching document this is still not the answer we just got the matching document right so retrieval augmented generation whenever we get the matching document this document needs to pass through with the question to the lm then only we will get the answer so let's go to the next step what we have vector store and red but this vector store we can't use this invoke method so we can't put vector store right inside the chain what we saw here right where it is here invoke we can't do that thing so we need to convert it into the retrieval object so the line chain makes it easy so we could use our vector store as a retriever and we will get the retriever object and we could do the same thing what we just did we can now use the invoke method and pass our query and it should do the similar job of the same job or it just did okay now we saw we got this two documents written I guess yeah this is one document this is another document and both of them are against same so this retriever is capable of retrieving the information now let's put this retriever with our chain so that we can improve our chain so I'm creating on a chat prompt template here let's see similar to this so we want to have a template here which will have a place for the question and the context that we are getting so we are seeing answer the question base only on the following context so there is a place folder for the context and there is a place folder for the question so let's you know run this prompt now let's create the rack chain first we will create this chain ourselves and then basically we will see what pre-built chain we got from the line chain what is in our chain you see this one new thing here I will explain you what exactly that mean but look at the chain chain has a two components here the first component is this dictionary what you see here and this dictionary is going to the prompt you don't see any enemy here because we are just putting a retriever and we want to see whether we can get this dynamic context and dynamic questions or not whether the prompt is getting popularly and we know that every chain we could invoke irrespective of whether we have you know LLM prompt or something so let me run this thing and I will show you what we see okay so we got the prompt template value so basically we got this same template with the matching context and the question okay you can see we got those documents dynamically added at the context here this one what exactly this doing here so we know that our prompt requires two place folder the question and the context you know what does runnable pass through runnable pass is nothing but whatever we are passing here that value is actually this runnable pass through so this part of the chain is saying whatever I am passing through while invoking the chain it becomes the question and the context will become whatever we get from the retriever that's why the prompt will get access to this two variable the context and the question let's say what we see inside the you know length so let me go there which one we ran let's say this one the first point whatever the input we pass it became a question this is what I guess this runnable pass through did it simply take our question this input what we pass and it became a question because the question is the variable they are expecting next vector store so vector store this query whatever we ask it goes to the vector store and it retrieved the two documents document one and the document two both from this doc x file after this vector store we could see a prompt is okay this runnable pass through yeah basically it is doing the same thing our input is just converted into that query that's what it happened here and this both thing happened parallelly that's what this mean actually here this represent is you know both the steps that we got this input it became the question and we got this documents back and then this documents and this question should be used in the chat prompt template correct this is the question and this is the documents they are going inside the chat prompt template and we got a message with the content answer the question based on the following context here context are our list of documents that we just pass and finally there should be a question that we pass so we got this prompt dynamically generated based on the whatever the input one thing if you notice we are passing this document objects right rather what we want we want this contain and not this object so we should be able to pass only this content attribute and now so let's handle that thing let me go back to the you know change and we create one small function which take our documents and it take the content page content of that particular document and join them is in this you know two new line thing so basically if I have this two document this two document will become a single stream where each document content is separated by this two slash right that's what we gonna do here let's run this one and now we just modified our chain what we want whenever we get you know context from retriever it should go through our this post processing function and then only it get assigned to the context and that's what we gonna do here this step this all things are same only instead of directly passing retriever here we are taking retriever it is going through our doc to string which is nothing but taking this document object converting into the single string and then it becomes the context and then then it goes to the prompt let's see whether it works check from template now you could see the content is a plain string and it's a single big string till the point here because both the documents got merged by this slash and double slash whatever call you know two new line character and then become a single and we should be able to see the same thing inside our let's click here and whether we see the same thing again this is a random parallel job what we saw right it takes this input now you see context is very plain text what we just pass and so this is the vector store we got this two document if you look at the doc to string it took those two documents and convert into the single long string separated by those two new line character this is what I like about the line chain especially this line smith which makes lm application development very easy because we could really figure it out why something is you know not working now let me go back to the outcode till the point we are still not reached to this particular thing you know which is basically generating answer using that retrieval we are still here only you know but now we are able to have the prompt which has the relevant document and the question detail now it's time to add the lm to that chain as I told you we are building you know step by step it should be now I have the I can call it now right chain where this part was already there we saw we got the retrieval output we pass it to the doc string then it becomes the context question was already there then we converted into the prompt prompt goes through now lm this prompt which we saw question answering prompt this one this will go to the lm and then then it will go finally your output parser and this is our rack a question answering on your document and we got here the final answer that it was founded in 2010 based on the document that we provided not we reached this point let's go into the langsmith now we could see there are a couple of more components or the you know we saw earlier it was up to this point or day we know up to this point you know this chart prompt let get then it goes to the lm you know this whole big message that we sent where we have our system instruction then we have our context up to this point and then finally we have our question and we are anchoring it for give me the answer we got the a m s s that a m s s should be come in here and we got the plain text output this is the retrieval augmented generation right we generated our output where we augmented our lm with the context that we retrieved from our vector store which is our own proprietary data and that's how we built you know the rack application using let's say the line chain chain this is still a question answering it is not a conversation because it will not be able to answer the follow up questions and we need to do something you know to answer the follow up question that's what we're going to see the next now okay now let's see how can we make our rack application which is able to answer you know the question that we are asking considering our own documents you know it is able to retrieve things but it is still a question answering basically okay if I ask something a follow up question there is no way or there is no mechanism to maintain our chat history so that it understand what we are doing so to make the conversation on drag or make it this question answering as a chat bot first thing we want to do is to introduce the chat history okay so we're going to create the chat history so first up on chat history is going to be the list of messages we have empty messages and then this is where our those custom now you know classes are going to be useful because now we could create the chat history you know by passing those messages okay so where is our first question so here we ask some question here and we got this response here let's put this question and response inside this chat history right so we're going to extend is basically we're going to add this list where we have these two messages first is the human message the question we ask and the AMS is the response we got and let's create the chat history okay so chat history is nothing but our list of messages where first message was this human message what we put in now this is important now we will see how we can pass this chat history so that we can make it more conversation and how it can handle the follow up question what exactly follow up question mean let's say the first question I ask when was this green grow innovations founded this is the first question if I ask second question like you know where it is headquartered this is a follow up question because I'm not mentioning you know about whom or about what entity I'm talking about this is a follow up question I'm seeing where it is headquartered it means there must be some earlier context to it that's why I'm able to have a follow up question imagine if I only take this follow up question which is where it is headquartered and try to find the matching document so let's go back to this diagram if I take where it is headquartered and I got documents about different different companies maybe they have their different headquartered information so basically if I only ask where it what is that thing where it is headquartered it means out of all this document it can find any document that match with the headquarter because it don't know which headquarter I'm talking about which company I'm talking about that's why it is very much important in the drag application if it is a conversational rat to take the question and contextualize it so basically that question needs to become a complete question that can be used to find the relevant document and that's what we're gonna do here so in the in the language and terminology we are calling it as a contextualizing our question so basically how do we contextualize we have to use LLM for that so what we could do we can take our question we can take our chat history and we can give it to LLM and say hey here is our chat history and here is the question that we have can you make this question okay can you you know make it a stand-alone question means it is a complete question that can be used so this is a simple prompt where we are saying this is the system instruction we are saying you know we will give you the chat history and the latest user question convert this into the stand-alone question and we will see whether how does you know it so basically we have this prompt now since we want to pass our chat history which is going to be the list of messages and we don't know this message type it could be anything sometimes it could be human message or system message or whatever we could use something called message placeholder so this message placeholder can take the list of messages dynamic sometimes it is two sometimes five whatever your chat history so this placeholder is name is called chat history and here we can pass our chat history and then the question that you know the user has asked this is the follow-up question and this prompt can convert this follow-up question into the complete question let's see whether it is able to contextualize it or not we are this is not the rag sorry this is not that final question answering this is the only to contextualizing or query your question let's ask this question where it is headquarter through our contextualized chain this contextualize prompt what we got we passed through the lnm so that lnm answers based on this prompt and then finally we are string out for partial right the only thing change instead of our question answering prompt we have this contextualized prompt let's run this chain so that we can see its purpose of contextualizing when we say where it is headquartered it converted into the complete question saying that where is the green grow innovations headquarter why because it has access store chat history we are passing this chat history when we are invoking you can see what we are passing we are passing the input we are passing the chat history let's go and check into the lines okay this is our contextualizing prompt what we just saw where if you look at the overall full final thing you know effect like the higher you know output here I could see I pass no no this sums to be wrong yeah this is the higher level out of this one let me go back and see whether it is correct okay let's see what happened first did we ask main was green grow innovation founded directly we ask like this let's go back and see whether what's happening here we got this prompt we got this chat history yeah chat history is there did we run it or not yeah we ran it right where it is headquartered is it the latest one we are seeing no maybe let me refresh it's run it again whether it worked or not contextualize key chain invoke this one so here input is where it is headquartered so this should be input right if I see this thing in our let's refresh it once again yes yes this is what I will run so we ask where it is headquartered you know this is what we ask and then we also pass a chat history which is our human message and the AM message which was the earlier thing and then finally it converted into the complete question that where is green row innovations headquartered so what we saw here outside why we sit this as output AI green row innovation was on why we see the output of this thing so we ask this where it is headquartered with the runable sequence yeah where it is headquartered with the two messages which is our chat history and then we got this complete message this is what whole sequence it did which is got this complete message then let's go back here to the front it multiple time or something so this is a placeholder so just a single lm call and it has to be the one single thing anyway so we just saw whenever we are putting this one where it is headquartered it is getting converted into the complete question now let's take this further we can create a retriever which is history our basically which is able to contextualize our chain or the contextualize our question so line chain as a pre-built chain called create history our chain this create history our chain requires extra thing called contextualize queue prompt and this all in so basically we just saw it we could use our contextualize prompt pass through the all in so we require these two extra things before we pass the retriever and that's what this create history our retriever does it take our normal retriever then uses our lm and our contextualize prompt and then we became the history of a retriever so basically retriever which takes the complete question and we can test it now so we ask where it is headquartered and you could see this retriever return this two document document one and document two all are related to the green grow it means this retriever was able to you know what I say this retriever what happens if I use the normal retriever let's say not the history our we also have the normal retriever right let's use only normal retriever and see what with the same question but this retriever do not require this all this thing right maybe it require only this thing you know this this okay so normal retriever should be able to answer this thing where it is headquarter let's say what we got see this is the problem if I only ask where it is headquarter it is just matching with any headquarter we got something company called quantum next and then this take wave something right that is why the contextualizing question is important since we are here contextualizing that question using this contextualize prompt it get converted into the complete question and that's the reason we are getting the matching documents from this right that is why it is very much important if you just pass the follow up question without contextualizing it you can get anything that is matching you know there so let's see we just ran this thing again I want to see in the line space whether everything is fine and we understand what's happening let's go to the line space okay so is it creating two runs or one run so this is one single thing right the history of a retriever the ultimate goal should be the message that we are getting let me go there this is what this is our retriever chain that we just ran now what it did the first step here what it is it has our you know input where it is headquartered and then it is so it is bus I guess it is just that you know taking this input runable sequence this is the next thing where we got everything executed but inside this chat prompt template so the first thing is the template the prompt template right that's what we saw this prompt should be getting constructed first thing and we got this chat history we got this prompt given a chat history you want to do something but in the other way we could see it I was seeing other right the default anyway route put looks same so we got this messages where we are containing a given the chat history do do this thing this was the you know what is the better way to look at this okay I think messages this is the first message yeah this is the first message which is our system instruction then next is human message which is our history you know human message this is the answer to that first message and this is the new message that we ask where it is headquartered that is nothing but our prompt is now getting generated which is this one a system instruction then our chat history we just saw and then the human input now this goes to a delim where it will contextualize so given a chat history which is this three messages we just saw coming from the chat prompt template and we got the AMS as which is where is the green road and then okay it's going through this parser basically you know plain text and then this goes to the retriever so we saw when retriever runs it is taking the contextualized query and not the original query the original query was what the original query is the where it is headquartered but when the retriever runs it has taken the contextualized query that's why we got the proper documents that's why it is very much important to contextualize I used to call it as a refined query but now I also call it let's say contextualize because that's what language and drivers now we are able to contextualize but things are not finished here because we just did this part now you know till the point we are only doing this retrieve part only thing we depresses now we have the conversational support that this retrieve is capable of dealing with the follow-up questions and bringing the right document you know here with an extra llm call that we just saw now let's put all these things together to create the final lm chat okay this is just that retriever in what we did right that leave that thing now let's create one more prompt because the earlier prompt we just saw was it is was for the contextualizing now we require this qa prompt again so that it is that you know for helpful assistant we will give you the context we will give you the chat history this is new because we're gonna pass the chat history to it also and then it's gonna answer the our question but this time we're gonna use the Lantern pre-built capabilities one chain we could create using these you know the pre-built chain is called the stop document chain which is basically the question answering chain where we require an lm and our question answering prompt lm is the same instance that we have initiated in a long time and this qa prompt so basically question answering prompt can take this whole this thing you know chat history and this is the final element in our this thing we are talking about this one this is what we just created which is this stop chain and we can create the final retrieval chain which is basically whole this thing by using the one which we just created which is this question answering chain and this whole part we just executed earlier is called the context average retrieval you know that thing so we take our sorry history of a retrieval then we take the question answering chain and then we call the final fraction so the difference is here we are using all this pre-built chain no one so earlier what we did if you forget what we did earlier earlier we used to see this is our qa chain this is what we did earlier our qa chain and this was the retrieval part now we replace that functionality with pre-built chain this create stop chain which takes our lm and the qa prompt the question answering prompt which has the placeholder for context input wearing chat history and we know that this is not sufficient because this question answering will only work well when the right context will come and the right context will only come when we will use a history of a retriever so these both things combined together we are creating a retrieval chain which has this history aware retrieval plus the lm prompt let us run this thing now so that we could see in the line space that's where we could see easily what's happening you also see that when we use this pre-built chain the output is coming like everything what input we pass what chat history we pass and then I think what the context we got and there is a final answer we got it has everything so it does not so one of the good reason to use this chain because you have all this debugging information at your end what was the input what was the chat history and this is the final now it is able to answer that green grow innovation is headquarter in Portland or even there it is headquartered so this is actually the right chain it is taking the relevant document considering the follow up question and actually answering it rather than just returning the document let me go back to the again our you know thing so this is our full retrieval chain single chain okay we saw it has multiple components into it the first component when I click on it what its job first component take where it is headquartered and give me the relevant documents and the history that's the job the first component what is the second component it second component took our query the relevant documents our history and then finally it answered the thing and we could say step by step for example first this prompt or you know that this should be our contextualized prompt given a chat history all this chat history we have given you know this is a prompt creation then this is actually calling that contextualized prompt bypassing the lm and they got the contextualized query where is it then it goes to vector database we got the relevant documents here now all of those things because we know the query chain required the context chat history and the input everything goes there so the query chain which is this one it got everything input chat history and the documents and then finally it is able to answer it here this is the you know query prompt this is the dynamic context that we got from this vector retriever and finally this is the chat history earlier messages this is our follow up question and we got this here you might be wondering okay here in the vector store if you see we have used the stand alone question a refined question to get the document but in the final query column we are using that same follow up question and it work here why because this lm is a chat model and it has already access to my chat history that's why even if I ask the follow up question it knows what I'm asking the problem was not with the lm whether it is able to answer the follow up question or not problem was actually getting the right relevant documents that was the challenge whether if I ask the follow up question whether I'm able to get the retrieval in the proper documents or not and that is the reason we are using that history our way of thinking and that's why when I call the kVA prompt here I'm just passing the original input which is follow up question still make it the right answer because lm can deal with the follow up question once it has the right context that's how you know be to think okay till the point okay we have got you know basically the conversation drag it is able to answer the follow up question by contextualizing you bring the right document and we saw how we can use the history aware retrieval and we also saw how we could create the retrieval chain by using that history aware retrieval and the you know whatever the question and sharing chain now final thing in the first part of our crash course is actually how do we you know this make it more production really right no one has stored the history like this you know a python variable that what we just saw right we need to use database to store all this conversation and we should be able to retrieve that you know the conversation history basically those all chat messages based on which conversation going on because at a time there will be multiple users using you know this application and for each user or each session we need to store this thing and that is the next thing we're going to do in the collaborative tool so real quick because we're going to see this in the second part when we actually put this thing as a part of first API so as I mentioned we require some database to store what I'm going to do I'm going to use the SQLite database so that I could support the multiple user so I'm going to create some database called drag app here you know we are using SQLite which is in memory database SPLite I'm defining one function a create application log what it does it simply create one table and that table I'm calling application log which is storing a session ID which we will create because every time conversation is happening we have to give some session or conversation ID that's why we will come to the which message is part of which conversation so that is the session ID this is the user query that is asked this is the GPT response we got LLM response we got which model did we use and this is a typical simple fields we use any time we build any application to log all these details next there is this function which take a session ID which we're going to create what user ask what response we got and simply insert into this particular table so you got table created you got you know function to insert now we need a function to get that chat history so we have another function called gate chat history we take the session ID and retreats all the messages of that particular session so we let's say use a wear clause wear session is equal to whatever we are passing and we want to get what user ask and what GPT got right so whatever the user ask which is user query will become the human message and whatever the GPT response we got which will become the AI message and we will collect all of messages here and that's how we will get the history very simple so let's create this application lock table right and see whether it is working or not and as I told you we require a single session ID to that particular conversation so that the because when in real application it will work there will be multiple users will be chatting you know and they might be having multiple sessions and this will be more clear when we go to the fast API because then you will thinking like an backend how do I generate that session ID when should I generate how do I know whether it is part of the same conversation or not that happens here but let's say for simplicity okay I got one message and it is a fresh message because let's say when they sent me a message the session ID was empty it means there is no ongoing session so I created a unique ID using this library a session ID and I want to get some chat history for it for currently I am just printing to show you and let me ask now this is my current question when was the green grow innovations funded let's run this thing so that things become more clear okay you see the chat history is empty because this is the new session and for that session there is no information in the database and I ask when was the green grow innovations founded and this is what the human message which is what I was and this is the green group now if you see I have inserted this detail now inserted database in insert application lock I have put this session ID I have put the question that is ask the answer I got because that's what we saw it can take the you know session user query jeopardy response in the model that we use now we got some history now let's ask a second question which is a follow-up question where it is held quarter so whenever we get such a follow-up question through our API the UI will pass this session ID so when we give our first response this we should also send this session ID to the UI we will see this thing as I'm telling you we will build this API so that you understand it better for now I'm just saying whenever you respond you have to pass this session ID to the UI via API so that next time when it asks this follow-up question it will give you this follow-up question but it will also pass the same session ID which is part of this conversation and if you use that same session ID again here then you will find there are some messages in your database now what we are building you can see the chat history of our two messages first message was you know when was this founded and its corresponding answer and this is my second question which I just asked where it is held quarter and it is able to answer correctly using our Rackchain this is the same Rackchain that we have been using on the top so what we just did rather than maintaining a temporary list for the you know maintaining those messages we created a database table and whenever we answer a user query we insert that record in the database as one record and we also give the session ID to it so that if we gave the second question using that same session ID then we can retrieve the history and our Rack will use that history to contextualize this question and then answer us maybe we can see inside the prompt that it is doing the same thing yeah not the prompt you know what I'm saying I think it's clear we don't need to go here again and again basically you know it is doing that same thing only thing is that history we hand love using our this thing now imagine now new user comes how do we know it's a new user if API receives a request without any session ID let's say what happens here I'm at the chat GPT if I ask something let's say I'm asking this so first question was write a hello now I am in the same thread I type this and it is giving me hallowed programming javascript because you understand the follow up question because it knows the first question was about writing the hallowed python and it has given me the hallowed python the second if I'm saying javascript it means I'm actually asking about the hallowed so this is the thing but asking the follow up question because I'm part of the same thread now I'm not sure whether I should click here because I got so many threads maybe there are some personal naming to those threads so I don't want to go there but this is what single thread but if this what we just did is actually this one we what we did here so we maintain the session ID and we use that same session ID for the second question this is what we did here but if I click this icon now new chat basically I don't have any session ID so I'm going to make a request with the fresh message and if it is a new message or let's say new user we will again generate the session ID we will give it back so that next time it should send us right this is how you could maintain a database that can handle a you know conversation further you could even maintain the user ID if you want here so that you know you which user has which conversation we should definitely maintain the user ID also right this is how you could use a persistent store you know something like you know a database here I use SQL ID could use my SQL proofs graceful you know whatever you want to you could use it and a lot of people have these questions whenever I you know create this rack tutorial they want to know okay how does it will handle multiple conversation you know how does we will come to know whether which message is to pick up so this is what you have to create a unique identifier for that conversation which is the session ID and using that you know you can basically what I say retrieve the relevant messages from the database right so this was our first part of our you know crash course hey everyone welcome back to our line chain conversational rack crash course this is the second part of our course in the first part we have built a conversational rack basically a rack chatbot inside this colab notebook and this particular chatbot is able to handle the follow questions you know we are able to contextualize or refine those user questions bring the right documents we have seen how to use the prebuilt line chain chains and we also saw how can we maintain a chat history inside the database so we have all the working parts here but as we know we not going to deploy you know this colab notebook when we want to deploy any application so basically we will be creating an API like this so what we will do in this video in this video we going to convert this colab notebook into the API and we will also see how can any front end like a streamlit application or it could be any react JS next JS application how it can communicate with our API right and especially we will understand how we are able to maintain the session so that you know we are able to answer the follow question inside that particular conversation so let us do one thing first first we will you know see what is this API and what it offers but refer that if you are not familiar with this part first part I you know I encourage you to please go and watch this particular part because this is where I have explained all the code that we are going to use so I will be restructuring the same code to create this API so let's see what this API offers so first API you could see there is an endpoint further chat endpoint which basically we can chat with our rag or let's say we can you know chat with our own documents that's what this endpoint is next endpoint we have to upload the doc you know we can add more documents to our rag application and keep updating the knowledge base we could list our documents and then we can delete the documents what we have so let's see first let's list document to see what we have seems to be there is no document okay we don't have any documents so this is a fresh rag and let's see what happens if I ask question to this rag which doesn't have any documents so let me ask those same questions here session is optional we will see what exactly the session so see what is session if you use the chat GPT if you click here a new chat a new session start so here let's say if I don't provide the session ID it means that you know I don't have any other conversation history I'm starting fish and servers should create a session ID and give me back so let's see we are asking when was this green grow innovations founded and it is saying it is founded in 2016 and it has also retained a session ID we know that we haven't provided any documents so this answer 2016 is actually coming from its own memory we haven't provided any document if you want to ask a follow up questions then we should pass this session ID but let's say we are not passing the follow up question we are still asking the same question again so we saw it was saying 2016 let's ask again this time it is saying 2014 so it is trying to you know every time come up with some answer okay this time again it is saying 2014 right but you can see the session ID should be different let's say this is 65 so every time this now it is 2015 so every time you ask a question without specifying the session ID what our back end is doing it is starting a session for you and it is giving you the session ID so that you can ask the follow up question one thing we understand if we ask something and charty pity don't have answered for it it can gaze or hallucinate you know it can come up with its own answer and it could be inconsistent in all the time that is why we want to give our own knowledge to it let's upload a document so we know that we have a document let me upload the document so let's upload the document we got let's upload this history document which has this answer and now we have uploaded this file it is uploaded successfully we could confirm whether the file is uploaded or not let's list our files yes we could see the file did upload now let's go back and ask the same question now we know because answer should be 2010 or something that's what the the answer should be so let's go and check now this time it is saying correct now it is able to answer from our own document and it also has given us session ID let's use this session ID and ask the follow up question let's ask it without you know follow up question and let's see what happens sorry without session ID and let's see what happens I'm gonna ask we are not passing the session ID so it's going to be like a first session then the first message in the session it is saying Portland green do is in now this answer is correct now you might be wondering okay without even any session ID it is able to you know answer the follow up question it is able to answer the follow up question because in our document there is only one company information you know this green grow company information and that is the only document our system is able to find and that's why it is able to answer correctly even though you know it doesn't know which companies you know we are asking for let's do one thing okay let's put some more documents you know about the different company and now let's see whether it is able to answer correctly without specifying so I got a couple of company document let's say I got this one company let me upload that doc let me try the other one also and so I think we got now four companies info now let's see what happens we're gonna ask the same question to confirm whether we got all the documents let's go to the least docs and we should be able to see all the documents which are uploaded here yeah so we got now four companies information now let's see what happens if I ask the same question okay now it is failing to answer now it is talking about this quantum next thing but my first question was what the green grow so it failed to answer because there are now more documents which is matching the word headquarter or so matching something semantically with the headquarter right that is why it is very much important to maintain that session and handle or you know contextualize that you know that's what we saw in the video now let's do this time property so let's ask our question right now let's take this session ID and then ask the follow question this time if I use this session ID it can retrieve my previous message from the database and should be able to answer correctly okay now it is able to answer that it is you know green grow because it knows the first message was related to green grow this is how the session is playing the role session is helping us to find out our previous messages from the database and use those messages to contextualize this particular you know question and we could also see that thing in the lines mean just like what we saw last time so the first time we asked this question right and you could see there are multiple things what we have and if you look at our LLM it has only one message but if you look at the next call which we made which is subsequent follow call it should have more messages it should also have a history see it has our first question first answer and this is our second question it is able to get this session IDs because we are able to send those things you know in the API because we are maintaining a session ID and that's what we want to see how do we maintain the session ID and we can also see we can delete the document and that information won't be accessible so let's list and delete I think we already know the ID so the ID number one is this file let's delete it okay let's go where is the delay talk by the it requires an ID so file ID was one I'm going to delete now that file so this file got deleted let's confirm whether it got deleted yeah we got only now three files here now let's make sure whether it is also deleted from the rag now we will ask a question and see whether it is able to answer or whether it is able to get the relevant document from our system see there is no context about the green group because we have deleted document it means deletion is also working properly by the way if you are not familiar with the first API or something you could check you know I have created a video on the first API that's a beginner guide that you can you know watch so let's now we saw the API right let's look at the front end also which is communicating with this API before we go inside the code so here is the stream lid application that can communicate with our API and we could ask the same questions here also so because this stream lid is calling our API now this is the chart and this is the you know the left side panel and you know if I do the refresh documents here we can see here three files coming this refresh document is basically listing our document it is calling our least document endpoint and it is showing it there for the delet also it is using same list and we could delete but before that we can ask that same question okay why it is taking time because I am using GPT4 I should use the GPT4 mini it is saying it doesn't have any information that's good if you look at when I loaded first time stream lid chart there is no session it's the start of the conversation and we can check the API logs what exactly it has seen so this is the API logs and we could just check this is the request that you know came and you could see the session ID was none and our chatbot our API has given a new session ID so that this session ID should come next time if it wants to communicate inside the same session now let's ask something here let's I'm just asking something hi hello and let's observe the API logs this is our request came high this time you can see it has passed the session ID which we have given earlier so this was the session ID provided by the API and it has seen so that that is how it is able to have the conversation inside the same thread sorry and it got some response now since this document is not there let's upload this document right if I refresh it will be again a new session will start and we will see how we are doing it in a stream lid chart so let me upload a document that we just deleted which is this one upload document now we should be able to ask the question so this document is uploaded successfully I don't want to I want to get it from me and it should be able to answer it is able to answer now we can check the first time it sends a message the session ID should be none this is the session ID is none and now it should send this session ID for the subsequent messages because you know API gives us back the session ID when it gives a response whenever you don't send the session ID and if you pass the session ID you will get that same session ID back let's look at here yes it is able to answer the follow up question and we could see now the request here is the request this request has sent the session ID that was required which is given by the back end API and this is how it is able to have the conversation so multiple users can interact whenever they start a new session whenever they refresh the new session ID gets generated for that particular conversation okay so let's get into the code so we will start with the stream lid code first because we just saw the API end points and let's say someone exposes this API to us we could use it so what you see here on the you know stream lid the right side part is where you see the stream lid chat or some you know a conversation window and the left side panel we got some model selection drop down we got some upload document section we have some section to you know list of documents and we could even delete one of these documents from here now let's look at the code so I have structure code here you know API and the app let me go inside the app which is the stream lid app here the stream lid app as I mentioned it has two section the sidebar and the chat interface what we just saw so whenever this stream lid application loads it does two things it initialize the messages array as a session state variable with the empty list and the session as a none that's how we are able to send that non-session so whenever you load your window or whenever we initialize this application the session state gets variable you know called session ID which is none and list of messages will be also empty because you are the first time loading there will not be any messages now let's look at first sidebar if you go inside the sidebar as I mentioned we have a model selection drop down as a sidebar next there is an upload document section you can see here this section is related to the upload document here we have file uploader from the stream lid again I'm not going to explain the stream lid to you you can watch the other video to know about the stream lid better than the high level this is what we are doing you know we are using the stream lid file uploader and whenever someone clicks on that button this particular upload button when someone clicks on this upload button we are calling our API and if I click on upload document I'm coming here to something called API utils so here we have written all the utility functions for our stream lid you know app API so there is an upload document function which takes our file which is uploaded on the stream lid and simply calling our API using the python request library very simple this is how we are calling the you know upload document to upload to our API next section you could see this is where we are listing our documents here which was this section listing documents will be also simply calling our list documents what is list documents function so list document function this one let me assume it maybe yeah this function what it is doing it is calling our list document endpoints and in returning those documents here and we are saying and finally whatever the documents we have written we are iterating through documents right and then we are calling the delayed endpoint where it is delayed here right so whenever this select we get this selected file id and if they click on a delayed document button we simply call our delayed document endpoint we just require the selected file id so this is our drop down whenever you select it will have our selected file id when you click on the delayed document we are calling our delayed document endpoint and basically it is you know just calling our delayed top and after deleting what we are doing you know we are just refreshing our documents again because once we delayed we need to refresh our document so the code is very pretty self explanatory you know code and don't worry you will you will get this code from me so you don't need to remember you know this thing I will be giving this code also when I publish the video so just we look at the sidebar which is simply calling our APIs for a particular use case now important part is the chat interface the conversation you know part here let's look at that code which is inside this chat interface file once again if you look at our stimulate app it is loading the sidebar code from display sidebar this is the sidebar.py and for the chat interface we are loading the code from the chat interface so I just structure it in a that way so chat interface first thing what you see on the chat is the list of conversations list of messages of our conversation and where they are stored they are stored in your session because we saw this messages you know in the session array so first your code snippet is simply going through all the messages we have and displaying those messages using the streamed chat message that's what it does next is this input window where user can put their prompt or their input that we need to answer and this is nothing but this part so whenever there is some message in this you know this thing we will also display that message look if I say something see we are displaying and then we are calling the API to generate the response and whenever we get an response we just display that thing also that's what here happening we are calling an API this is where our chat endpoint we are calling right this is our chat endpoint we are calling by passing whatever the question user has and we are also passing the session from where we are getting this session because we store in the session state you know stream lit session state and first time this session will be the empty because that's what we do when we initialize this application the session is none and we are sending that detail this is typical calling you know and this part is nothing but that extra information that we are showing just like a debugging information if you want to show you know this way so very simple code I would say you know pretty self explanatory code just calling our own APIs you know this is how the front end days will call if they are using some next years or you know react days to build some you know chatbot we chat or something they will be calling our API like this so this was our stream lit code now let's look at our API code you know and that API code is nothing but the whole code what we had written here I just restructure that code so let's look at the what kind of files do we have so first thing we have is an main.pv which is our entry point all these endpoints we have specified in this particular file so you can see we have a chat endpoint we have upload doc endpoint we have list documents and the delayed document these are the endpoints what we got here in this particular thing then what we did if your family with the first API we know that we can declare some you know a pidentic model to specify the schema now here we got one schema let's say whenever the chat endpoint get called we are expecting an input of type query input so this is where we define the schema for everything that what we are expecting and that's how these things get rendered here you know that what we are expecting you know this variable names and all you see in the swag of now those are all defined inside a file called pidentic models so this where I put all of our you know the pidentic models so this is for the query input when we receive input from the user we are expecting a question from the user we are expecting the session ID which is by default it will be none so it could be optional and the model should be one of this that is what we have mentioned here and default we have put GPT for a mini that's what this query input what it is specify we have also specified structured for the response so in the response we are giving back the answer we are also giving the session ID and the model that is we use which is again a model is a model of type this and we have seen this already that when we get response we get answer session and the model this is what we specify so these are the pidentic schema which bring some structure to our application this is what we expect for the you know input and this is what we are giving the output and then we also have some other schema specified you know for the other end point for example this is for the document info we will see first of all these are the schemas we have been mentioned next we got a nanchin utility this is where we put some more lanchin prompt and the rack chain that we have created inside this colab notebook we will go through the code first I just want to cover the files what we got so we got the lanchin utility this is database utility right we know we are maintaining the history inside the database and this particular file has all our db function right creating our table inserting application the recording to maintaining the chat history and all sorts of things are happening here and then finally we got the promo utility basically all things related to the you know vector search indexing splitting those document that code is here which is also again came from our colab notebook now let's go step by step we will cover each end point code let me go back to the main file and let's focus on the chat end point what's happening which is this one up to this point now in the chat end point as I mentioned whenever someone calling the chat end point this function get invoked right and it is expecting the query input we just saw it's thing then we take a session ID from it we are just logging this information in our law and if there is no session ID provided we generate the session ID you know that's why you get the session ID back next we try to see for this particular session ID do we have any chat history so if we if I go to the chat history it is the same function we saw into the colab notebook it is simply checking our application lock stable and see whether we have any messages for the session ID that is provided that's what the chat history does and when we get the chat history we get our rack chain and rack chain has that same code that we you know saw in the first part where give me minute sorry yeah so here the rack chain is taking the modern image input and then basically we declare our NLM here we are using this you know history aware retriever which require an NLM retriever and also contextualize from this is where we are handling the follow up question and the retriever together that's why we are calling the history aware you know retriever and that's what we saw last time history aware retriever I guess yeah here history aware retriever where it is this is what we saw earlier that is the same thing I'm using and then we are using the pre chain you know prebuilt chain called create stuff documents chain this one which takes you know creates this is the question and selling chain basically this is the same chain we saw yesterday it takes the NLM and the prompt and we get the question and then we can combine now this is the retriever with contextualizing for follow up questions this is the question answering and then we now combine together to create our final rack chain and we are just retaining that chain here that's the small thing this function is doing and we got the chain and we simply invoke this chain with the input what user has passed and the chat history what we have and finally we returning the answer in the response this is where we are returning the query response that's what the chat endpoint is so most of the things what we saw yesterday are actually you know you know here you know let's look at what happens so when we get the answer we also want to maintain the chat history because if we get a subsequent follow up message is inside the same session we should be able to retrieve that information that's where when we get the answer now we are inserting before giving the response we are inserting the details in the database using this function insert application log so insert application log so we have table called the application logs where adding the session ID what user ask what jippity response and which model was used that's what you know we did here next let's see the other end point what we got the upload dock endpoint here what we are doing at a high level you know we are expecting a file to be uploaded we are checking some type that we only allow this file you could have your own logic you know here and then basically we write that file to a temporary location why I am writing file to the temporary location basically this utility we are using from the line chain it is expecting a path file path so we are just you know writing that file to the temporary path and then two things are happening first we insert that record inside our database table also and then we insert into the chroma DB so let's look at the insert document record if you look at insert document record using a different table called document store so let's look at once again what utility functions we have so we we have seen this function yesterday so we got function to get the database connection this function create our application log table and this function when we just saw create our document store table that we just gonna you know use and the insert document record basically take the file name and insert into the document by giving some incremental ID and we get some file ID this is the same file ID we saw when we are dealing with the listing of the documents you know this file ID this file ID and the file name coming from this document store table and we are using the same information even to delete that particular file so let's say once we insert into the table that details next thing we want to do want to make sure we index that document we take that file path and we take this file ID why file ID I will show you so if I go to the index document first thing we you know use that same logic what we have to do use we kind of take that file split into the smaller documents and then each splits are nothing but those documents we are adding a metadata filter it we are adding a file ID a one metadata filter why because when we call a delayed endpoint we want to delete from the document store table but we also want to delete from the vector store to know which chunks to delete what we do whatever the file ID we pass into the delayed document right we delete all the chunks associated and that's why we want to make sure that we know which chunks belong to which file ID and that's the reason we are storing this metadata you could store any kind of metadata and finally we are adding those document to the vector store that's what you know this part did insert into the table insert into the vector database and we delayed the file what is this delayed document record okay okay if we don't get success from the Chromadb we also delayed from our documents store to just maintain the you know consistency and finally we delayed the temporary file that we created because we don't need that file now this was all let me zoom out okay so this is whole function was actually uploading the document the next thing is simple the listing document is simply calling our you know get all documents getting the list of document from our document store and return a back to us thing and finally delayed document which again has to do two thing delayed from the Chromadb and the delayed from the document store so first we try to delete from the Chromadb delayed from the Chromadb we have the file ID that user has seen we first of all we are just printing how many chunks we have and eventually from the vector store we are deleting all the chunks where the file ID is matching with this particular file so this is where this metadata field at what we have stored is helping us so we are matching a metadata field called file ID and we are deleting all those record associated or all the chunks associated with this particular file ID and once the Chromadb delayed is successful we also delayed this from our document store and you know simply calling the delayed from let's say this SQL query and deleting from the document so though we haven't written this code line by line but the code is pretty you know easy that you know you can understand so basically this is how we you know converted our colab notebook which we had here into some kind of a structured fast API application where you have interpoints where we have stored all the structure associated with you know okay I think we missed this the document info let's look at this one okay so when you list docs you are specifying what is the response so response going to be the list of this document info this is the structure for our you know response the response is some list of document info document info is one single object and how what we have specified document info is nothing but each document will have ID it's a file name and when it was uploaded and that's why we are able to get this thing here right so this is all nothing but the defining the structure for your request and the response right and we should follow this thing whenever you know we build application that you know we define a or a kind of a data type this are the identity models that is also helpful to understand you know what is expected and what is the expected response we got a gate so yeah so we have this identity model these are all our entry point data you know APN points are here Lanchin utility is be kept here the database utility is be kept here and the chroma utility is be you know kept here and this is just the application log that we are you know logging so at startup or application I added one statement here you know to log this thing and I think we are just logging this endpoint because I wanted to see whether we are getting the proper session ID is that very you know so this is what you know we have we have already seen how to you know use this API endpoint basically you know this is streamed data application I'm running in the environment so running is very easy you just use the jubicon I am the name of the file where you are you know API what is that entry point which is this main file and then app is the name of the first API app what we have declared and the reload is if something I change this in the code it just restart again similarly this is our streamed application which is nothing but simply streamed run the name of the file where the streamed application which is inside this streamed app and I could simply run this thing okay so I hope you got the idea how we converted our you know the Colagno book into the first API the important thing is actually here to make sure that you know that we create the session ID whenever user gives us the first question and front end should also say in that same session ID if they want to have the follow-up question inside the that same conversation and a lot of the people you know I'm not exactly aware how do we build the things you know chatbot for the multiple users or how does it remember the conversation so this is what we do we store the conversation history inside the database each conversation get conversation ID or the session ID that helps us to retrieve their previous messages and send those previous messages to the LLM in our case it's an open AI you know chatgipity what we are using hope you find this you know useful and I will also share the Colagno book and this the stream written API code that you know you guys just saw let me know inside the comment if you have any you know doubts and that I can clarify if you require any other material or if you you know have any suggestions for the video thank you